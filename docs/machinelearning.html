<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Machine Learning (placeholder) | The WIPO Patent Analytics Handbook</title>
  <meta name="description" content="This is a working draft of the WIPO Patent Analytics Handbook." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Machine Learning (placeholder) | The WIPO Patent Analytics Handbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a working draft of the WIPO Patent Analytics Handbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Machine Learning (placeholder) | The WIPO Patent Analytics Handbook" />
  
  <meta name="twitter:description" content="This is a working draft of the WIPO Patent Analytics Handbook." />
  

<meta name="author" content="Paul Oldham" />


<meta name="date" content="2019-10-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="references.html"/>
<link rel="next" href="classification.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/d3-4.10.2/d3.min.js"></script>
<script src="libs/sankey-1/sankey.js"></script>
<script src="libs/sankeyNetwork-binding-0.4/sankeyNetwork.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.2/leaflet.js"></script>
<link href="libs/collapsibleTree-0.1.6/collapsibleTree.css" rel="stylesheet" />
<script src="libs/collapsibleTree-binding-0.1.7/collapsibleTree.js"></script>

<script>
/* ========================================================================
 * Bootstrap: transition.js v3.3.7
 * http://getbootstrap.com/javascript/#transitions
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // CSS TRANSITION SUPPORT (Shoutout: http://www.modernizr.com/)
  // ============================================================

  function transitionEnd() {
    var el = document.createElement('bootstrap')

    var transEndEventNames = {
      WebkitTransition : 'webkitTransitionEnd',
      MozTransition    : 'transitionend',
      OTransition      : 'oTransitionEnd otransitionend',
      transition       : 'transitionend'
    }

    for (var name in transEndEventNames) {
      if (el.style[name] !== undefined) {
        return { end: transEndEventNames[name] }
      }
    }

    return false // explicit for ie8 (  ._.)
  }

  // http://blog.alexmaccaw.com/css-transitions
  $.fn.emulateTransitionEnd = function (duration) {
    var called = false
    var $el = this
    $(this).one('bsTransitionEnd', function () { called = true })
    var callback = function () { if (!called) $($el).trigger($.support.transition.end) }
    setTimeout(callback, duration)
    return this
  }

  $(function () {
    $.support.transition = transitionEnd()

    if (!$.support.transition) return

    $.event.special.bsTransitionEnd = {
      bindType: $.support.transition.end,
      delegateType: $.support.transition.end,
      handle: function (e) {
        if ($(e.target).is(this)) return e.handleObj.handler.apply(this, arguments)
      }
    }
  })

}(jQuery);
</script>
<script>
/* ========================================================================
 * Bootstrap: collapse.js v3.3.7
 * http://getbootstrap.com/javascript/#collapse
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */

/* jshint latedef: false */

+function ($) {
  'use strict';

  // COLLAPSE PUBLIC CLASS DEFINITION
  // ================================

  var Collapse = function (element, options) {
    this.$element      = $(element)
    this.options       = $.extend({}, Collapse.DEFAULTS, options)
    this.$trigger      = $('[data-toggle="collapse"][href="#' + element.id + '"],' +
                           '[data-toggle="collapse"][data-target="#' + element.id + '"]')
    this.transitioning = null

    if (this.options.parent) {
      this.$parent = this.getParent()
    } else {
      this.addAriaAndCollapsedClass(this.$element, this.$trigger)
    }

    if (this.options.toggle) this.toggle()
  }

  Collapse.VERSION  = '3.3.7'

  Collapse.TRANSITION_DURATION = 350

  Collapse.DEFAULTS = {
    toggle: true
  }

  Collapse.prototype.dimension = function () {
    var hasWidth = this.$element.hasClass('width')
    return hasWidth ? 'width' : 'height'
  }

  Collapse.prototype.show = function () {
    if (this.transitioning || this.$element.hasClass('in')) return

    var activesData
    var actives = this.$parent && this.$parent.children('.panel').children('.in, .collapsing')

    if (actives && actives.length) {
      activesData = actives.data('bs.collapse')
      if (activesData && activesData.transitioning) return
    }

    var startEvent = $.Event('show.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    if (actives && actives.length) {
      Plugin.call(actives, 'hide')
      activesData || actives.data('bs.collapse', null)
    }

    var dimension = this.dimension()

    this.$element
      .removeClass('collapse')
      .addClass('collapsing')[dimension](0)
      .attr('aria-expanded', true)

    this.$trigger
      .removeClass('collapsed')
      .attr('aria-expanded', true)

    this.transitioning = 1

    var complete = function () {
      this.$element
        .removeClass('collapsing')
        .addClass('collapse in')[dimension]('')
      this.transitioning = 0
      this.$element
        .trigger('shown.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    var scrollSize = $.camelCase(['scroll', dimension].join('-'))

    this.$element
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)[dimension](this.$element[0][scrollSize])
  }

  Collapse.prototype.hide = function () {
    if (this.transitioning || !this.$element.hasClass('in')) return

    var startEvent = $.Event('hide.bs.collapse')
    this.$element.trigger(startEvent)
    if (startEvent.isDefaultPrevented()) return

    var dimension = this.dimension()

    this.$element[dimension](this.$element[dimension]())[0].offsetHeight

    this.$element
      .addClass('collapsing')
      .removeClass('collapse in')
      .attr('aria-expanded', false)

    this.$trigger
      .addClass('collapsed')
      .attr('aria-expanded', false)

    this.transitioning = 1

    var complete = function () {
      this.transitioning = 0
      this.$element
        .removeClass('collapsing')
        .addClass('collapse')
        .trigger('hidden.bs.collapse')
    }

    if (!$.support.transition) return complete.call(this)

    this.$element
      [dimension](0)
      .one('bsTransitionEnd', $.proxy(complete, this))
      .emulateTransitionEnd(Collapse.TRANSITION_DURATION)
  }

  Collapse.prototype.toggle = function () {
    this[this.$element.hasClass('in') ? 'hide' : 'show']()
  }

  Collapse.prototype.getParent = function () {
    return $(this.options.parent)
      .find('[data-toggle="collapse"][data-parent="' + this.options.parent + '"]')
      .each($.proxy(function (i, element) {
        var $element = $(element)
        this.addAriaAndCollapsedClass(getTargetFromTrigger($element), $element)
      }, this))
      .end()
  }

  Collapse.prototype.addAriaAndCollapsedClass = function ($element, $trigger) {
    var isOpen = $element.hasClass('in')

    $element.attr('aria-expanded', isOpen)
    $trigger
      .toggleClass('collapsed', !isOpen)
      .attr('aria-expanded', isOpen)
  }

  function getTargetFromTrigger($trigger) {
    var href
    var target = $trigger.attr('data-target')
      || (href = $trigger.attr('href')) && href.replace(/.*(?=#[^\s]+$)/, '') // strip for ie7

    return $(target)
  }


  // COLLAPSE PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this   = $(this)
      var data    = $this.data('bs.collapse')
      var options = $.extend({}, Collapse.DEFAULTS, $this.data(), typeof option == 'object' && option)

      if (!data && options.toggle && /show|hide/.test(option)) options.toggle = false
      if (!data) $this.data('bs.collapse', (data = new Collapse(this, options)))
      if (typeof option == 'string') data[option]()
    })
  }

  var old = $.fn.collapse

  $.fn.collapse             = Plugin
  $.fn.collapse.Constructor = Collapse


  // COLLAPSE NO CONFLICT
  // ====================

  $.fn.collapse.noConflict = function () {
    $.fn.collapse = old
    return this
  }


  // COLLAPSE DATA-API
  // =================

  $(document).on('click.bs.collapse.data-api', '[data-toggle="collapse"]', function (e) {
    var $this   = $(this)

    if (!$this.attr('data-target')) e.preventDefault()

    var $target = getTargetFromTrigger($this)
    var data    = $target.data('bs.collapse')
    var option  = data ? 'toggle' : $this.data()

    Plugin.call($target, option)
  })

}(jQuery);
</script>
<script>
window.initializeCodeFolding = function(show) {

  // handlers for show-all and hide all
  $("#rmd-show-all-code").click(function() {
    $('div.r-code-collapse').each(function() {
      $(this).collapse('show');
    });
  });
  $("#rmd-hide-all-code").click(function() {
    $('div.r-code-collapse').each(function() {
      $(this).collapse('hide');
    });
  });

  // index for unique code element ids
  var currentIndex = 1;

  // select all R code blocks
  var rCodeBlocks = $('pre.sourceCode, pre.r, pre.python, pre.bash, pre.sql, pre.cpp, pre.stan, pre.js');
  rCodeBlocks.each(function() {

    // create a collapsable div to wrap the code in
    var div = $('<div class="collapse r-code-collapse"></div>');
    if (show)
      div.addClass('in');
    var id = 'rcode-643E0F36' + currentIndex++;
    div.attr('id', id);
    $(this).before(div);
    $(this).detach().appendTo(div);

    // add a show code button right above
    var showCodeText = $('<span>' + (show ? 'Hide' : 'Code') + '</span>');
    var showCodeButton = $('<button type="button" class="btn btn-default btn-xs code-folding-btn pull-right"></button>');
    showCodeButton.append(showCodeText);
    showCodeButton
        .attr('data-toggle', 'collapse')
        .attr('data-target', '#' + id)
        .attr('aria-expanded', show)
        .attr('aria-controls', id);

    var buttonRow = $('<div class="row"></div>');
    var buttonCol = $('<div class="col-md-12"></div>');

    buttonCol.append(showCodeButton);
    buttonRow.append(buttonCol);

    div.before(buttonRow);

    // update state of button on show/hide
    div.on('hidden.bs.collapse', function () {
      showCodeText.text('Code');
    });
    div.on('show.bs.collapse', function () {
      showCodeText.text('Hide');
    });
  });

}
</script>
<script>
/* ========================================================================
 * Bootstrap: dropdown.js v3.3.7
 * http://getbootstrap.com/javascript/#dropdowns
 * ========================================================================
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 * ======================================================================== */


+function ($) {
  'use strict';

  // DROPDOWN CLASS DEFINITION
  // =========================

  var backdrop = '.dropdown-backdrop'
  var toggle   = '[data-toggle="dropdown"]'
  var Dropdown = function (element) {
    $(element).on('click.bs.dropdown', this.toggle)
  }

  Dropdown.VERSION = '3.3.7'

  function getParent($this) {
    var selector = $this.attr('data-target')

    if (!selector) {
      selector = $this.attr('href')
      selector = selector && /#[A-Za-z]/.test(selector) && selector.replace(/.*(?=#[^\s]*$)/, '') // strip for ie7
    }

    var $parent = selector && $(selector)

    return $parent && $parent.length ? $parent : $this.parent()
  }

  function clearMenus(e) {
    if (e && e.which === 3) return
    $(backdrop).remove()
    $(toggle).each(function () {
      var $this         = $(this)
      var $parent       = getParent($this)
      var relatedTarget = { relatedTarget: this }

      if (!$parent.hasClass('open')) return

      if (e && e.type == 'click' && /input|textarea/i.test(e.target.tagName) && $.contains($parent[0], e.target)) return

      $parent.trigger(e = $.Event('hide.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this.attr('aria-expanded', 'false')
      $parent.removeClass('open').trigger($.Event('hidden.bs.dropdown', relatedTarget))
    })
  }

  Dropdown.prototype.toggle = function (e) {
    var $this = $(this)

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    clearMenus()

    if (!isActive) {
      if ('ontouchstart' in document.documentElement && !$parent.closest('.navbar-nav').length) {
        // if mobile we use a backdrop because click events don't delegate
        $(document.createElement('div'))
          .addClass('dropdown-backdrop')
          .insertAfter($(this))
          .on('click', clearMenus)
      }

      var relatedTarget = { relatedTarget: this }
      $parent.trigger(e = $.Event('show.bs.dropdown', relatedTarget))

      if (e.isDefaultPrevented()) return

      $this
        .trigger('focus')
        .attr('aria-expanded', 'true')

      $parent
        .toggleClass('open')
        .trigger($.Event('shown.bs.dropdown', relatedTarget))
    }

    return false
  }

  Dropdown.prototype.keydown = function (e) {
    if (!/(38|40|27|32)/.test(e.which) || /input|textarea/i.test(e.target.tagName)) return

    var $this = $(this)

    e.preventDefault()
    e.stopPropagation()

    if ($this.is('.disabled, :disabled')) return

    var $parent  = getParent($this)
    var isActive = $parent.hasClass('open')

    if (!isActive && e.which != 27 || isActive && e.which == 27) {
      if (e.which == 27) $parent.find(toggle).trigger('focus')
      return $this.trigger('click')
    }

    var desc = ' li:not(.disabled):visible a'
    var $items = $parent.find('.dropdown-menu' + desc)

    if (!$items.length) return

    var index = $items.index(e.target)

    if (e.which == 38 && index > 0)                 index--         // up
    if (e.which == 40 && index < $items.length - 1) index++         // down
    if (!~index)                                    index = 0

    $items.eq(index).trigger('focus')
  }


  // DROPDOWN PLUGIN DEFINITION
  // ==========================

  function Plugin(option) {
    return this.each(function () {
      var $this = $(this)
      var data  = $this.data('bs.dropdown')

      if (!data) $this.data('bs.dropdown', (data = new Dropdown(this)))
      if (typeof option == 'string') data[option].call($this)
    })
  }

  var old = $.fn.dropdown

  $.fn.dropdown             = Plugin
  $.fn.dropdown.Constructor = Dropdown


  // DROPDOWN NO CONFLICT
  // ====================

  $.fn.dropdown.noConflict = function () {
    $.fn.dropdown = old
    return this
  }


  // APPLY TO STANDARD DROPDOWN ELEMENTS
  // ===================================

  $(document)
    .on('click.bs.dropdown.data-api', clearMenus)
    .on('click.bs.dropdown.data-api', '.dropdown form', function (e) { e.stopPropagation() })
    .on('click.bs.dropdown.data-api', toggle, Dropdown.prototype.toggle)
    .on('keydown.bs.dropdown.data-api', toggle, Dropdown.prototype.keydown)
    .on('keydown.bs.dropdown.data-api', '.dropdown-menu', Dropdown.prototype.keydown)

}(jQuery);
</script>
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
.row { display: flex; }
.collapse { display: none; }
.in { display:block }
.pull-right > .dropdown-menu {
    right: 0;
    left: auto;
}
.open > .dropdown-menu {
    display: block;
}
.dropdown-menu {
    position: absolute;
    top: 100%;
    left: 0;
    z-index: 1000;
    display: none;
    float: left;
    min-width: 160px;
    padding: 5px 0;
    margin: 2px 0 0;
    font-size: 14px;
    text-align: left;
    list-style: none;
    background-color: #fff;
    -webkit-background-clip: padding-box;
    background-clip: padding-box;
    border: 1px solid #ccc;
    border: 1px solid rgba(0,0,0,.15);
    border-radius: 4px;
    -webkit-box-shadow: 0 6px 12px rgba(0,0,0,.175);
    box-shadow: 0 6px 12px rgba(0,0,0,.175);
}
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>


<script>
document.write('<div class="btn-group pull-right" style="position: absolute; top: 20%; right: 2%; z-index: 200"><button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true" data-_extension-text-contrast=""><span>Code</span> <span class="caret"></span></button><ul class="dropdown-menu" style="min-width: 50px;"><li><a id="rmd-show-all-code" href="#">Show All Code</a></li><li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li></ul></div>')
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The WIPO Patent Analytics Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Note to Readers</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="" data-path="acknowlegements.html"><a href="acknowlegements.html"><i class="fa fa-check"></i>Acknowlegements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="how-to-use-the-handbook.html"><a href="how-to-use-the-handbook.html"><i class="fa fa-check"></i>How to use the Handbook</a><ul>
<li class="chapter" data-level="0.0.1" data-path="how-to-use-the-handbook.html"><a href="how-to-use-the-handbook.html#datasets"><i class="fa fa-check"></i><b>0.0.1</b> Datasets</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i><b>2</b> Scientific Literature</a><ul>
<li class="chapter" data-level="2.1" data-path="literature.html"><a href="literature.html#accessing-the-scientific-literature"><i class="fa fa-check"></i><b>2.1</b> Accessing the Scientific Literature</a></li>
<li class="chapter" data-level="2.2" data-path="literature.html"><a href="literature.html#searching-literature-databases"><i class="fa fa-check"></i><b>2.2</b> Searching Literature Databases</a><ul>
<li class="chapter" data-level="2.2.1" data-path="literature.html"><a href="literature.html#stemming"><i class="fa fa-check"></i><b>2.2.1</b> Stemming</a></li>
<li class="chapter" data-level="2.2.2" data-path="literature.html"><a href="literature.html#using-search-operators"><i class="fa fa-check"></i><b>2.2.2</b> Using Search Operators</a></li>
<li class="chapter" data-level="2.2.3" data-path="literature.html"><a href="literature.html#proximity-operators"><i class="fa fa-check"></i><b>2.2.3</b> Proximity Operators</a></li>
<li class="chapter" data-level="2.2.4" data-path="literature.html"><a href="literature.html#regular-expressions"><i class="fa fa-check"></i><b>2.2.4</b> Regular Expressions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="literature.html"><a href="literature.html#precision-vs.recall"><i class="fa fa-check"></i><b>2.3</b> Precision vs. Recall</a></li>
<li class="chapter" data-level="2.4" data-path="literature.html"><a href="literature.html#processing-scientific-literature"><i class="fa fa-check"></i><b>2.4</b> Processing Scientific Literature</a></li>
<li class="chapter" data-level="2.5" data-path="literature.html"><a href="literature.html#visualizing-the-scientific-literature"><i class="fa fa-check"></i><b>2.5</b> Visualizing the Scientific Literature</a><ul>
<li class="chapter" data-level="2.5.1" data-path="literature.html"><a href="literature.html#dashboards"><i class="fa fa-check"></i><b>2.5.1</b> Dashboards</a></li>
<li class="chapter" data-level="2.5.2" data-path="literature.html"><a href="literature.html#network-visualisation"><i class="fa fa-check"></i><b>2.5.2</b> Network Visualisation</a></li>
<li class="chapter" data-level="2.5.3" data-path="literature.html"><a href="literature.html#other-forms-of-visualisation"><i class="fa fa-check"></i><b>2.5.3</b> Other forms of visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="literature.html"><a href="literature.html#linking-the-scientific-literature-with-patent-analysis"><i class="fa fa-check"></i><b>2.6</b> Linking the Scientific Literature with Patent Analysis</a><ul>
<li class="chapter" data-level="2.6.1" data-path="literature.html"><a href="literature.html#mapping-authors-to-inventors"><i class="fa fa-check"></i><b>2.6.1</b> Mapping Authors to Inventors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="literature.html"><a href="literature.html#linking-citations-with-patent-literature"><i class="fa fa-check"></i><b>2.7</b> Linking Citations with Patent Literature</a></li>
<li class="chapter" data-level="2.8" data-path="literature.html"><a href="literature.html#conclusion"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="patents.html"><a href="patents.html"><i class="fa fa-check"></i><b>3</b> Counting Patent Data</a><ul>
<li class="chapter" data-level="3.1" data-path="patents.html"><a href="patents.html#the-structure-of-patent-numbers"><i class="fa fa-check"></i><b>3.1</b> The structure of patent numbers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="patents.html"><a href="patents.html#the-country-code"><i class="fa fa-check"></i><b>3.1.1</b> The country code</a></li>
<li class="chapter" data-level="3.1.2" data-path="patents.html"><a href="patents.html#the-numeric-identifier"><i class="fa fa-check"></i><b>3.1.2</b> The numeric identifier</a></li>
<li class="chapter" data-level="3.1.3" data-path="patents.html"><a href="patents.html#kind-codes"><i class="fa fa-check"></i><b>3.1.3</b> Kind Codes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="patents.html"><a href="patents.html#preparing-to-count-patent-data"><i class="fa fa-check"></i><b>3.2</b> Preparing to Count Patent Data</a></li>
<li class="chapter" data-level="3.3" data-path="patents.html"><a href="patents.html#counting-priority-or-first-filings"><i class="fa fa-check"></i><b>3.3</b> Counting Priority or First Filings</a></li>
<li class="chapter" data-level="3.4" data-path="patents.html"><a href="patents.html#counting-priority-applications"><i class="fa fa-check"></i><b>3.4</b> Counting Priority Applications</a></li>
<li class="chapter" data-level="3.5" data-path="patents.html"><a href="patents.html#counting-applications"><i class="fa fa-check"></i><b>3.5</b> Counting Applications</a><ul>
<li class="chapter" data-level="3.5.1" data-path="patents.html"><a href="patents.html#mapping-publications-family-members"><i class="fa fa-check"></i><b>3.5.1</b> Mapping Publications (Family Members)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="patents.html"><a href="patents.html#trends-by-country-using-publication-data"><i class="fa fa-check"></i><b>3.6</b> Trends by Country using Publication Data</a></li>
<li class="chapter" data-level="3.7" data-path="patents.html"><a href="patents.html#families"><i class="fa fa-check"></i><b>3.7</b> Patent Families</a></li>
<li class="chapter" data-level="3.8" data-path="patents.html"><a href="patents.html#forecasting-patent-activity"><i class="fa fa-check"></i><b>3.8</b> Forecasting Patent Activity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="patstat.html"><a href="patstat.html"><i class="fa fa-check"></i><b>4</b> PATSTAT (placeholder)</a><ul>
<li class="chapter" data-level="4.0.1" data-path="patstat.html"><a href="patstat.html#getting-started"><i class="fa fa-check"></i><b>4.0.1</b> Getting Started</a></li>
<li class="chapter" data-level="4.0.2" data-path="patstat.html"><a href="patstat.html#recipe-1"><i class="fa fa-check"></i><b>4.0.2</b> Recipe 1</a></li>
<li class="chapter" data-level="4.0.3" data-path="patstat.html"><a href="patstat.html#recipe-2-ipc-based-analysis"><i class="fa fa-check"></i><b>4.0.3</b> Recipe 2 IPC based analysis</a></li>
<li class="chapter" data-level="4.0.4" data-path="patstat.html"><a href="patstat.html#recipe-3-patent-family-analysis"><i class="fa fa-check"></i><b>4.0.4</b> Recipe 3 Patent Family Analysis</a></li>
<li class="chapter" data-level="4.0.5" data-path="patstat.html"><a href="patstat.html#recipe-4-applicants"><i class="fa fa-check"></i><b>4.0.5</b> Recipe 4 Applicants</a></li>
<li class="chapter" data-level="4.0.6" data-path="patstat.html"><a href="patstat.html#recipe-5-inventors"><i class="fa fa-check"></i><b>4.0.6</b> Recipe 5 Inventors</a></li>
<li class="chapter" data-level="4.0.7" data-path="patstat.html"><a href="patstat.html#recipe-6-non-patent-literature"><i class="fa fa-check"></i><b>4.0.7</b> Recipe 6 Non Patent Literature</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="indicators.html"><a href="indicators.html"><i class="fa fa-check"></i><b>5</b> Indicators (placeholder)</a></li>
<li class="chapter" data-level="6" data-path="textmining.html"><a href="textmining.html"><i class="fa fa-check"></i><b>6</b> Text Mining (placeholder)</a></li>
<li class="chapter" data-level="7" data-path="geocoding.html"><a href="geocoding.html"><i class="fa fa-check"></i><b>7</b> Geocoding</a><ul>
<li class="chapter" data-level="7.0.1" data-path="geocoding.html"><a href="geocoding.html#getting-started-1"><i class="fa fa-check"></i><b>7.0.1</b> Getting Started</a></li>
<li class="chapter" data-level="7.0.2" data-path="geocoding.html"><a href="geocoding.html#getting-set-up-with-the-google-maps-api"><i class="fa fa-check"></i><b>7.0.2</b> Getting set up with the Google Maps API</a></li>
<li class="chapter" data-level="7.0.3" data-path="geocoding.html"><a href="geocoding.html#using-the-api"><i class="fa fa-check"></i><b>7.0.3</b> Using the API</a></li>
<li class="chapter" data-level="7.0.4" data-path="geocoding.html"><a href="geocoding.html#the-source-data"><i class="fa fa-check"></i><b>7.0.4</b> The Source Data</a></li>
<li class="chapter" data-level="7.1" data-path="geocoding.html"><a href="geocoding.html#lookup-the-records"><i class="fa fa-check"></i><b>7.1</b> Lookup the Records</a><ul>
<li class="chapter" data-level="7.1.1" data-path="geocoding.html"><a href="geocoding.html#using-placement"><i class="fa fa-check"></i><b>7.1.1</b> Using placement</a></li>
<li class="chapter" data-level="7.1.2" data-path="geocoding.html"><a href="geocoding.html#using-ggmap"><i class="fa fa-check"></i><b>7.1.2</b> Using ggmap</a></li>
<li class="chapter" data-level="7.1.3" data-path="geocoding.html"><a href="geocoding.html#using-googleway"><i class="fa fa-check"></i><b>7.1.3</b> Using Googleway</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="geocoding.html"><a href="geocoding.html#reviewing-initial-results"><i class="fa fa-check"></i><b>7.2</b> Reviewing Initial Results</a><ul>
<li class="chapter" data-level="7.2.1" data-path="geocoding.html"><a href="geocoding.html#tackling-abbreviations"><i class="fa fa-check"></i><b>7.2.1</b> Tackling Abbreviations</a></li>
<li class="chapter" data-level="7.2.2" data-path="geocoding.html"><a href="geocoding.html#lookup-edited-names"><i class="fa fa-check"></i><b>7.2.2</b> Lookup edited names</a></li>
<li class="chapter" data-level="7.2.3" data-path="geocoding.html"><a href="geocoding.html#bringing-the-data-together"><i class="fa fa-check"></i><b>7.2.3</b> Bringing the data together</a></li>
<li class="chapter" data-level="7.2.4" data-path="geocoding.html"><a href="geocoding.html#assessing-the-quality-of-geocoding"><i class="fa fa-check"></i><b>7.2.4</b> Assessing the Quality of Geocoding</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="geocoding.html"><a href="geocoding.html#preprocess-the-data-and-rerun-the-query"><i class="fa fa-check"></i><b>7.3</b> Preprocess the Data and Rerun the Query</a><ul>
<li class="chapter" data-level="7.3.1" data-path="geocoding.html"><a href="geocoding.html#duplicated-affiliation-names"><i class="fa fa-check"></i><b>7.3.1</b> Duplicated Affiliation Names</a></li>
<li class="chapter" data-level="7.3.2" data-path="geocoding.html"><a href="geocoding.html#quickly-mapping-the-data"><i class="fa fa-check"></i><b>7.3.2</b> Quickly Mapping the Data</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="geocoding.html"><a href="geocoding.html#round-up"><i class="fa fa-check"></i><b>7.4</b> Round Up</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>8</b> References</a></li>
<li class="chapter" data-level="9" data-path="machinelearning.html"><a href="machinelearning.html"><i class="fa fa-check"></i><b>9</b> Machine Learning (placeholder)</a><ul>
<li class="chapter" data-level="9.0.1" data-path="machinelearning.html"><a href="machinelearning.html#artificial-intelligence-machine-learning"><i class="fa fa-check"></i><b>9.0.1</b> Artificial Intelligence &amp; Machine Learning</a></li>
<li class="chapter" data-level="9.0.2" data-path="machinelearning.html"><a href="machinelearning.html#word-vectors"><i class="fa fa-check"></i><b>9.0.2</b> Word Vectors</a></li>
<li class="chapter" data-level="9.0.3" data-path="machinelearning.html"><a href="machinelearning.html#word-vectors-with-fastext"><i class="fa fa-check"></i><b>9.0.3</b> Word Vectors with fastext</a></li>
<li class="chapter" data-level="9.0.4" data-path="machinelearning.html"><a href="machinelearning.html#training-word-vectors-for-drones"><i class="fa fa-check"></i><b>9.0.4</b> Training Word Vectors for Drones</a></li>
<li class="chapter" data-level="9.0.5" data-path="machinelearning.html"><a href="machinelearning.html#using-word-vectors"><i class="fa fa-check"></i><b>9.0.5</b> Using Word Vectors</a></li>
<li class="chapter" data-level="9.0.6" data-path="machinelearning.html"><a href="machinelearning.html#exploring-analogies"><i class="fa fa-check"></i><b>9.0.6</b> Exploring Analogies</a></li>
<li class="chapter" data-level="9.0.7" data-path="machinelearning.html"><a href="machinelearning.html#patent-specific-word-embeddings"><i class="fa fa-check"></i><b>9.0.7</b> Patent Specific Word Embeddings</a></li>
<li class="chapter" data-level="9.0.8" data-path="machinelearning.html"><a href="machinelearning.html#machine-learning-in-classification"><i class="fa fa-check"></i><b>9.0.8</b> Machine learning in Classification</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>10</b> Patent Classfication (placeholder)</a></li>
<li class="chapter" data-level="11" data-path="citations.html"><a href="citations.html"><i class="fa fa-check"></i><b>11</b> Patent Citations</a><ul>
<li class="chapter" data-level="11.0.1" data-path="citations.html"><a href="citations.html#non-patent-literature"><i class="fa fa-check"></i><b>11.0.1</b> Non Patent Literature</a></li>
<li class="chapter" data-level="11.0.2" data-path="citations.html"><a href="citations.html#literature-and-patent-citation-data-with-the-lens"><i class="fa fa-check"></i><b>11.0.2</b> Literature and Patent Citation Data with the Lens</a></li>
<li class="chapter" data-level="11.0.3" data-path="citations.html"><a href="citations.html#patent-citations"><i class="fa fa-check"></i><b>11.0.3</b> Patent Citations</a></li>
<li class="chapter" data-level="11.0.4" data-path="citations.html"><a href="citations.html#navigating-patent-networks"><i class="fa fa-check"></i><b>11.0.4</b> Navigating Patent Networks</a></li>
<li class="chapter" data-level="11.0.5" data-path="citations.html"><a href="citations.html#forward-citations"><i class="fa fa-check"></i><b>11.0.5</b> Forward Citations</a></li>
<li class="chapter" data-level="11.0.6" data-path="citations.html"><a href="citations.html#counting-citations-by-patent-families"><i class="fa fa-check"></i><b>11.0.6</b> Counting Citations by Patent Families</a></li>
<li class="chapter" data-level="11.0.7" data-path="citations.html"><a href="citations.html#citations-and-knowledge-spillovers"><i class="fa fa-check"></i><b>11.0.7</b> Citations and Knowledge Spillovers</a></li>
<li class="chapter" data-level="11.1" data-path="citations.html"><a href="citations.html#conclusion-1"><i class="fa fa-check"></i><b>11.1</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="social.html"><a href="social.html"><i class="fa fa-check"></i><b>12</b> Social Media and Patent Analytics (placeholder)</a></li>
<li class="divider"></li>
<li><a href="https://github.com/wipo-analytics/handbook" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The WIPO Patent Analytics Handbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machinelearning" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Machine Learning (placeholder)</h1>
<p></p>
<div id="artificial-intelligence-machine-learning" class="section level3">
<h3><span class="header-section-number">9.0.1</span> Artificial Intelligence &amp; Machine Learning</h3>
<!--- highlihgt the difference between word2vec, Glove, and fastext as they have some impotant differences notably fastext capturing out of word vocabularies. Reference gensim for word vector creation in python--->
<!--- Google patent embeddings on big query---->
<p><em>Please note this is a working draft and is likely to be substantially revised. Citations are incomplete</em></p>
<p>In recent years Artificial Intelligence has become a focus of discussion for its potentially transformative and disruptive effects on economy and society. The recent rise of artificial intelligence in the patent system is the focus of a landmark 2019 WIPI Technology Trends Report “Artificial Intelligence” <a href="https://www.wipo.int/publications/en/details.jsp?id=4386">https://www.wipo.int/publications/en/details.jsp?id=4386</a>. The in depth review of patent activity revealed that it is one of the fastest growing areas of patent activity with inventions that are applicable across diverse fields such as telecommunications, transportation, life and medical sciences, personal devices and human-computer interactions. Sectors identified in the WIPO report include banking, entertainment, security, industry and manufacturing, agriculture and networks. Perhaps the best known “flagship” initiative for artificial intelligence is the pursuit of self-driving cars by Tesla and Google among others. However, many companies, including those that work in the domain of patent analytics are increasingly claiming that they apply artificial intelligence as part of their products.</p>
<p>When approaching artificial intelligence it is important to look beyond the hype and marketing strategies to the the underlying technology. In practical terms this can be described as computer based approaches to classification with respect to images and texts. The dominant approach to classification involves a range of computational machine learning approaches that have been undergoing rapid development in recent years.</p>
<p>Examples of the use of machine learning approaches to classification tasks include predictive text entry on mobile phones, a technology anticipated in the 1940s by researchers in China [wikipedia]. A patent for an assistive device for deaf people involving predictive text was awarded in 1988 (US4754474A) [wikipedia]. The rise of mobile phones witnessed an explosion in the development and use of predictive text applications. Predictive text is also widely used in search engines to suggest phrases that a user may wish to use in their search. Other applications include spam filtering for emails or suggesting similar items that a customer might be interested in on online shops.</p>
<p>While text classification is perhaps the main everyday area where machine learning is encountered in practice image classification has been the major focus of development and is reflected in the prominence of image classification challenges on Kaggle. The implementation of image classification approaches is reflected in everyday terms in image searches in online databases which suggest relevant images and suggestions for tagging of images and persons in social media applications. Image classification is also an important area of innovation in areas such as medical diagnostics, robotics, self-driving cars or facial recognition for security systems. A separate but less visible area of development is control systems. The online data science platform <a href="https://www.kaggle.com/competitions">Kaggle</a> serves as a host for competitions and challenges in machine learning such as image classification and can provide an insight into the nature of machine learning developments.</p>
<p>A 2017 report by the UK Royal Society “Machine learning: the power and promise of computers that learn by example” provides a valuable overview of machine learning approaches.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>. For our purposes, the Royal Society report highlights the key underlying feature of machine learning: learning by example.</p>
<p>As we will see in this chapter machine learning approaches commonly involve training a statistical model to make <em>predictions</em> about <em>patterns</em> (in texts or in images). Training of machine learning models is normally based on the use of examples. The quality of the predictions that are produced by a model is heavily dependent on the quality and the number of examples that it is trained on.</p>
<p>The development of machine learning models proceeeds in a cycle from the collection and pre-processing of training data, to the development of the model with the training data followed by evaluation of the performace of the model against previously unseen data (known as the evaluation or test set). Based on the results more training data may be added and the parameters of the model adjusted or tuned to optimise performance. When a robust model has been developed it can then be used in production to automate the classification tasks.</p>
<p>Machine learning involves a range of different algorithms (that may at times be used in combination), examples include the well known Principal Component Analysis (PCA), linear regression, logistic regression (for classification), decisio-trees, K-means clustering, least squares and polynomial fitting, and neural networks of a variety of types (e.g convolutional, recurrent and feed forward). Some of the algorithms used in machine learning predate the rise in popularity of the term machine learning and would not be accepted as machine learnig (e.g. PCA and regression models). Readers interested in learning more about the algorithms involved in machine learning will discover a wide range of often free online machine learning courses such as from popular platorms such as Coursera, Udemy, Edx and Data Camp to name but a few. For text classification the Stanford Course “Stanford CS224N: NLP with Deep Learning” provides 20 hours of video lectures that provides a detailed insight into many of the topics addressed in this chapter.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<p>However, while it is important to engage with the background to machine learning algorithms, in reality machine learning is becoming increasingly accessible for a range of classification tasks in two ways.</p>
<ol style="list-style-type: lower-alpha">
<li>through fee based online cloud services offered by Google, Amazon, Microsoft Azure and others that will perform specific classification tasks at scale such as image classification without a requirement for advanced training;</li>
<li>the availability of free open source packages such as scikitlearn, fastText (Facebook), keras and spaCy (Explosion AI)</li>
</ol>
<p>One of the challenges writing about machine learning is making the processes involved visible. To make it easier to engage with machine learning we will use the free Python Natural Language Processing library <code>spaCy</code> and the associated fee based <code>Prodigy</code> annotation and model training tool from Explosion AI in Germany. While skikitlearn, fasttext and keras are also major tools, spaCy and Prodigy have the considerable advantage of allowing end to end transparency in writing about the steps innvolved in developing and applying machine learning models.</p>
<p>This chapter focuses on a basic machine learning workflow involving the following steps:</p>
<ol style="list-style-type: decimal">
<li>Creating seed terms from word vectors to build a text classification model, training the model and testing it.</li>
<li>Named Entity Recognition. Training a model to recognise entities of interest within the texts identified by the true or false model.</li>
<li>Using a model to classify and identify named entities in a text</li>
</ol>
</div>
<div id="word-vectors" class="section level3">
<h3><span class="header-section-number">9.0.2</span> Word Vectors</h3>
<blockquote>
<p>“You shall know a word by the company it keeps (Firth, J. R. 1957:11)”</p>
</blockquote>
<p>In the last chapter we explored approaches to text mining that do not involve the use of machine learning models. This involved text mining to identify terms for a dictionary that would allow for the identification of texts that contain one or more terms using ngrams.</p>
<p>Dictionary based methods provide powerful tools for analysis. However, they suffer from two main issues.</p>
<ol style="list-style-type: lower-alpha">
<li>they will only ever return exactly the same terms that are in the dictionary. That is, they cannot identify nearby or closely related terms.</li>
<li>dictionary based methods can be very memory intensive particularly if the dictionaries involved are very large.</li>
</ol>
<p>To overcome the first of these issues it is now common practice to combine dictionary approaches with a statistical model to more accurately classify texts at various levels of detail. That is to add a statistical learning component to improve classification.</p>
<p>In the case of the second issue it is important to bear in mind that machine learning models can be more demanding on memory and computational resources than dictionary based methods. However, as we will discuss below, the availability of machine learning models allows for the development of strategies to minimise demands on memory and computational power such as initially training a model with a very large dictionary and then deploying a purely statistical based model without the dictionary. Much however will depend on the precise task at hand and the available memory and compute power available. In this chapter we assume that you will be using a laptop with a modest 16Gb of Ram.</p>
<div id="word-vectors-1" class="section level4">
<h4><span class="header-section-number">9.0.2.1</span> Word Vectors</h4>
<p>In the last chapter we used Vantage Point to create a co-occurrence matrix to build search terms and to refine analysis of a dataset. A co-occurrence matrix can be built in Vantage Point either as a count of the number of times that words or phrases in a dataset co-occurr with each or using measures such as cosine similarity.</p>
<p>One straightforward way of thinking about a word vector is as a co-occurrence matrix where words are transformed into numeric values and the vocabulary is cast into a multidimensional vector space. Within that space words with the same or similar meanings will be closer (in terms of scores or weights). More precisely, words that <em>share similar contexts</em> will have the same or similar meanings while words with dissimmliar meanings will be further away. This observation is an important departure point for word vectors compared with a straightforward co-occurrence matrix that counts the number of times that words occur together in a given set. The reason for this is that the focus is on the context, or the company that a word is keeping. As we will see in a moment, word vectors are <em>learned representations</em> of the relationships between words based on minimisation of the loss (error) of a predictive model.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>(<a href="https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf" class="uri">https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf</a>)</p>
<p>The seminal paper on word vectors by Mikolov et al 2013 neatly summarises the problem they seek to address as follows:</p>
<blockquote>
<p>&quot;Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. <span class="citation">(<span class="citeproc-not-found" data-reference-id="mlens.org/104-512-929-235-758"><strong>???</strong></span>)</span></p>
</blockquote>
<p>The problem that Mikolov and co-authors identify is that the development of approaches such as automatic speech recognition is constrained by dependency on high quality manual transcripts of speech containing only millions of words while machine translation models are constrained by the fact that “…corpora for many languages contain only a few billions of words or less” <span class="citation">(<span class="citeproc-not-found" data-reference-id="mlens.org/104-512-929-235-758"><strong>???</strong></span>)</span>. Put another way, the constraint presented by approaches at the time was that the examples available for computational modelling could not accommodate the range of human uses of language or more precisely, the meanings conveyed. Mikolov et. al. successfully demonstrated that distributed representations of words using neural network based language models outperfornmed the existing Ngram models on much larger datasets (using 1 million common tokens from the Google News corpus) <span class="citation">(<span class="citeproc-not-found" data-reference-id="mlens.org/104-512-929-235-758"><strong>???</strong></span>)</span>.</p>
<blockquote>
<p>“We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].
Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].&quot;</p>
</blockquote>
<p>This observation has become one of the most famous in computational linguistics and is worth elaborating on. In a word vector it was found that.</p>
<p>King - Man + Woman = Queen</p>
<p>In a 2016 blog post on “The amazing power of word vectors” Adrian Coyler provides the following illustration of how this works. Note that the labels do not exist in the vectors and are added purely for explanation in this hypothetical example. <a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a> <!--- request copyright clearance to use the image---></p>
<p><img src="images/word2vec_coyler.png" width="328" /></p>
<p>Let us imagine that each word in each individual vector has a distributed value across hundreds of dimensions across the corpus. Words like King, Queen, Princess have a high similarity in vector space with the word Royalty. In contrast King has a strong similarity with Masculinity while Queen has a strong similarity with Femininity. Deducting Man from King and adding Woman can be readily be seen to lead to Queen in the vector space. Other well known examples from the same paper lead to the calculation that “big-bigger” = “small:larger” etc.</p>
<p>In follow up work Mikolov and others at Google released the word2vec tool for the creation of</p>
</div>
</div>
<div id="word-vectors-with-fastext" class="section level3">
<h3><span class="header-section-number">9.0.3</span> Word Vectors with fastext</h3>
<p>To illustrate the use of word vectors we will use the fastText machine learning package developed by Facebook.</p>
<p>FastText is a free text classification and representation package produced by Facebook that provides downloadable multi-language models for use in machine learning. At present vecotr models are available for 157 languages.</p>
<p>FastText can be used from the command line or in Python (fasttext) or in R with the <a href="https://github.com/pommedeterresautee/fastrtext">fastrtext package</a>. FastText is simple to install and use and is a good way to get started with word vectors and machine learning because it is very easy to use. Fasttext is under active development with the latest updates posted on the <a href="https://fasttext.cc/">fasttext website</a>.</p>
<p>FastText was developed by researchers including Tomas Mikolov as an alternative to the increasingly popular deep learning models for text classification at scale. It is a lightweight tool that emphasises speed in classification <span class="citation">(Bojanowski et al. <a href="#ref-bojanowski2016enriching">2016</a>; Joulin, Grave, Bojanowski, and Mikolov <a href="#ref-joulin2016bag">2016</a>; Joulin, Grave, Bojanowski, Douze, et al. <a href="#ref-joulin2016fasttext">2016</a>)</span> and arguably outperforms deep learning models.</p>
<p>To get started follow <a href="https://fasttext.cc/docs/en/support.html">the fastext instructions</a> to install fasttext from the command line or in Python. We will demonstrate fasttext in Python but it is easy, if not easier, to run from the command line.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb74-1" data-line-number="1">git clone https:<span class="op">//</span>github.com<span class="op">/</span>facebookresearch<span class="op">/</span>fastText.git</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">cd fastText</a>
<a class="sourceLine" id="cb74-3" data-line-number="3">sudo pip install .</a>
<a class="sourceLine" id="cb74-4" data-line-number="4"><span class="co"># or</span></a>
<a class="sourceLine" id="cb74-5" data-line-number="5">sudo python setup.py install</a></code></pre></div>
<p>Verify the installation</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb75-1" data-line-number="1">python</a>
<a class="sourceLine" id="cb75-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> fasttext</a>
<a class="sourceLine" id="cb75-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span></a></code></pre></div>
<p>If this has worked correctly you should not see anything after <code>import fasttext</code>.</p>
<p>In the Terminal we now need some data to train. The fastext example usses wikipedia pages in English that take up 15Gb. In the terminal download the smaller version as follows.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>mkdir data</a>
<a class="sourceLine" id="cb76-2" data-line-number="2"><span class="op">$</span><span class="st"> </span>wget <span class="op">-</span>c http<span class="op">:</span><span class="er">//</span>mattmahoney.net<span class="op">/</span>dc<span class="op">/</span>enwik9.zip <span class="op">-</span>P data</a>
<a class="sourceLine" id="cb76-3" data-line-number="3"><span class="op">$</span><span class="st"> </span>unzip data<span class="op">/</span>enwik9.zip <span class="op">-</span>d data</a></code></pre></div>
<p>As this is an XML file it needs to be parsed. The file for parsing is bundled with fastext as wikifil.pl and you will need to get the path right for your installation. If in doubt download fasttext from the command line, make and then cd into the directory for this step. Record the path to the file that you will need in the next step in Python.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1">perl wikifil.pl data<span class="op">/</span>enwik9 <span class="op">&gt;</span><span class="st"> </span>data<span class="op">/</span>fil9</a></code></pre></div>
<p>Check that the file has parsed on the command line.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1">head <span class="op">-</span>c <span class="dv">80</span> data<span class="op">/</span>fil9</a></code></pre></div>
<p>Train word vectors</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="im">import</span> fasttext</a>
<a class="sourceLine" id="cb79-2" data-line-number="2"><span class="co">#model = fasttext.train_unsupervised(&#39;/Users/colinbarnes/fastText/data/fil9&#39;)</span></a></code></pre></div>
</div>
<div id="training-word-vectors-for-drones" class="section level3">
<h3><span class="header-section-number">9.0.4</span> Training Word Vectors for Drones</h3>
<p>We will use a small set of patent texts on drones from the drones package for illustration. Ideally use the largest possible set. However, for better results use a single language and also regularise the texts, so that everything is lower case. You may also improve results by removing all punctuation.</p>
<p>If we wished to do that in R by way of example we access the title, abstract and claimes table (tac) in the drones training package. We would then combine the the fields, convert to lowecase and then replace all the punctuation with a space. We might tidy up by removing any double spaces created by removing the punctuation</p>
<!--- to do: provoe a pre processed file in the drones package-->
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb80-2" data-line-number="2"><span class="kw">library</span>(drones)</a>
<a class="sourceLine" id="cb80-3" data-line-number="3"> drones_texts_vec &lt;-<span class="st"> </span>drones<span class="op">::</span>tac <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb80-4" data-line-number="4"><span class="st">   </span><span class="kw">unite</span>(text, <span class="kw">c</span>(<span class="st">&quot;title_english&quot;</span>, <span class="st">&quot;abstract_english&quot;</span>, <span class="st">&quot;first_claim&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb80-5" data-line-number="5"><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_to_lower</span>(text)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb80-6" data-line-number="6"><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot; &quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb80-7" data-line-number="7"><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">text =</span> <span class="kw">str_replace_all</span>(text, <span class="st">&quot;  &quot;</span>, <span class="st">&quot; &quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb80-8" data-line-number="8"><span class="st">   </span><span class="kw">select</span>(text)</a>
<a class="sourceLine" id="cb80-9" data-line-number="9"> </a>
<a class="sourceLine" id="cb80-10" data-line-number="10"> <span class="kw">head</span>(drones_texts_vec)</a></code></pre></div>
<p>This cleaned up annonymised text is available in the data folder of this handbook and in the drones package <!--- TO DO--->. Note that patent texts can be messy and you may want to engage in further processing. Once we have the data in a cleaned cleaned up format we can pass it to fast text in the terminal.</p>
<p>There are two available models for word vectors in fast text. These are</p>
<ol style="list-style-type: lower-alpha">
<li>skipgrams (predict a word from the contexts words arount it)</li>
<li>cbow ADD DESCRIPTION</li>
</ol>
<p>Next in the terminal we navitage to the fastext folder and provide our csv or simple text file as an input and specify the output.</p>
<!--- THIS IS NOW THROWING AN ERROR AND NEEDS TO BE UPDATED--->
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>.<span class="op">/</span>fasttext skipgram <span class="op">-</span>input <span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>drones_vector_texts.csv <span class="op">-</span>output <span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>drones_vec</a></code></pre></div>
<p>There are total of 4.4 million words (tokens) in the vocabulary with xxx distinct words. It takes about 30 seconds for fasttext to process these words. These words boil down to 19,125 words in total.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">Read 4M words</a>
<a class="sourceLine" id="cb82-2" data-line-number="2">Number of words<span class="op">:</span><span class="st">  </span><span class="dv">19125</span></a>
<a class="sourceLine" id="cb82-3" data-line-number="3">Number of labels<span class="op">:</span><span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb82-4" data-line-number="4">Progress<span class="op">:</span><span class="st"> </span><span class="fl">100.0</span>% words<span class="op">/</span>sec<span class="op">/</span>thread<span class="op">:</span><span class="st">   </span><span class="dv">50624</span> lr<span class="op">:</span><span class="st">  </span><span class="fl">0.000000</span> avg.loss<span class="op">:</span><span class="st">  </span><span class="fl">1.790453</span> ETA<span class="op">:</span><span class="st">   </span>0h 0m 0s</a></code></pre></div>
<p>The processing creates two files in our target directory. The first is <code>drones_vec.bin</code> containing the model and the second is <code>drones_vec.vec</code>. The second file is actually a simple text file that contains the weights for each individual terms in the vector. Here is a glimpse of that file for the word <code>device</code>.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1">device <span class="fl">0.21325</span> <span class="fl">0.11661</span> <span class="fl">-0.060266</span> <span class="fl">-0.17116</span> <span class="fl">0.16712</span> <span class="fl">-0.03204</span> <span class="fl">-0.54853</span> <span class="fl">-0.30647</span> <span class="fl">0.023724</span> <span class="fl">-0.047807</span> <span class="fl">-0.068384</span> <span class="fl">-0.22845</span> <span class="fl">-0.08171</span> <span class="fl">0.046688</span> <span class="fl">0.26321</span> <span class="fl">-0.51804</span> <span class="fl">-0.02021</span> <span class="fl">0.099132</span> <span class="fl">-0.27856</span> <span class="fl">0.33479</span> <span class="fl">-0.027596</span> <span class="fl">-0.27679</span> <span class="fl">0.31599</span> <span class="fl">-0.32319</span> <span class="fl">0.048407</span> <span class="fl">-0.067782</span> <span class="fl">-0.086028</span> <span class="fl">0.070966</span> <span class="fl">-0.27628</span> <span class="fl">-0.43886</span> <span class="fl">-0.23275</span> <span class="fl">0.15364</span> <span class="fl">-0.037609</span> <span class="fl">0.16732</span> <span class="fl">-0.55758</span> <span class="fl">0.24021</span> <span class="fl">-0.21904</span> <span class="fl">-0.00074375</span> <span class="fl">-0.2962</span> <span class="fl">0.41962</span> <span class="fl">0.069979</span> <span class="fl">0.039564</span> <span class="fl">0.31745</span> <span class="fl">-0.11433</span> <span class="fl">0.15294</span> <span class="fl">-0.4063</span> <span class="fl">0.16489</span> <span class="fl">-0.17881</span> <span class="fl">-0.24346</span> <span class="fl">-0.17451</span> <span class="fl">0.19218</span> <span class="fl">-0.13081</span> <span class="fl">-0.052599</span> <span class="fl">0.12156</span> <span class="fl">-0.023431</span> <span class="fl">-0.066951</span> <span class="fl">0.19624</span> <span class="fl">0.11179</span> <span class="fl">0.17482</span> <span class="fl">0.34394</span> <span class="fl">0.17303</span> <span class="fl">-0.32398</span> <span class="fl">0.54666</span> <span class="fl">-0.30731</span> <span class="fl">-0.1117</span> <span class="fl">-0.017867</span> <span class="fl">0.081936</span> <span class="fl">-0.068579</span> <span class="fl">-0.15465</span> <span class="fl">0.057545</span> <span class="fl">0.026571</span> <span class="fl">-0.3714</span> <span class="fl">0.029978</span> <span class="fl">0.081706</span> <span class="fl">0.017101</span> <span class="fl">0.036847</span> <span class="fl">-0.13174</span> <span class="fl">0.24701</span> <span class="fl">-0.10006</span> <span class="fl">-0.11838</span> <span class="fl">-0.045929</span> <span class="fl">-0.13226</span> <span class="fl">0.20067</span> <span class="fl">0.12056</span> <span class="fl">0.43343</span> <span class="fl">0.052317</span> <span class="fl">-0.030258</span> <span class="fl">0.066875</span> <span class="fl">-0.1951</span> <span class="fl">0.12343</span> <span class="fl">0.031962</span> <span class="fl">-0.52444</span> <span class="fl">0.041814</span> <span class="fl">-0.64228</span> <span class="fl">0.13036</span> <span class="fl">0.040553</span> <span class="fl">0.30254</span> <span class="fl">-0.15474</span> <span class="fl">-0.57587</span> <span class="fl">0.29205</span> </a></code></pre></div>
<p>Note two points here. The first is that the default vector space is 100 dimensions but popular choices go up to 300. Note also that there will be common stop words (and, the, etc) in the model that we may want to remove. The second main point is that the file size of the .bin file is nearly 800Mb and may get much larger fast.</p>
<p>From the terminal we can print the word vectors for specific words as follows:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>echo <span class="st">&quot;drone autonomous weapon&quot;</span> <span class="op">|</span><span class="st"> </span>.<span class="op">/</span>fasttext print<span class="op">-</span>word<span class="op">-</span>vectors <span class="op">/</span>Users<span class="op">/</span>colinbarnes<span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>drones_vec.bin</a>
<a class="sourceLine" id="cb84-2" data-line-number="2">drone <span class="fl">0.38326</span> <span class="fl">0.4115</span> <span class="fl">0.28873</span> <span class="fl">-0.35648</span> <span class="fl">-0.24769</span> <span class="fl">-0.22507</span> <span class="fl">0.18887</span> <span class="fl">0.012016</span> <span class="dv">0</span>.<span class="dv">51823</span>...</a>
<a class="sourceLine" id="cb84-3" data-line-number="3">autonomous <span class="fl">0.41683</span> <span class="fl">0.39242</span> <span class="fl">0.16987</span> <span class="fl">-0.028905</span> <span class="fl">0.38609</span> <span class="fl">-0.57572</span> <span class="fl">-0.44157</span> <span class="dv">-0</span>.<span class="dv">51236</span>...</a>
<a class="sourceLine" id="cb84-4" data-line-number="4">weapon <span class="fl">0.20932</span> <span class="fl">0.59608</span> <span class="fl">0.21891</span> <span class="fl">-0.42716</span> <span class="fl">0.19016</span> <span class="fl">-0.76555</span> <span class="fl">0.23395</span> <span class="fl">-0.63699</span> <span class="dv">-0</span>.<span class="dv">12079</span>...</a></code></pre></div>
</div>
<div id="using-word-vectors" class="section level3">
<h3><span class="header-section-number">9.0.5</span> Using Word Vectors</h3>
<p>One common use of word vectors is to build a thesaurus. We can also check how our vectors are performing, and adjust the parameters if we are not getting what we expect. We do this by calculating the nearest neighbours (nn) in the vector space and then entering a query term, The higher the score the closer the neighout is.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>.<span class="op">/</span>fasttext nn <span class="op">/</span>Users<span class="op">/</span>colinbarnes<span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>drones_vec.bin</a>
<a class="sourceLine" id="cb85-2" data-line-number="2">Query word? drone</a>
<a class="sourceLine" id="cb85-3" data-line-number="3">codrone <span class="fl">0.69161</span></a>
<a class="sourceLine" id="cb85-4" data-line-number="4">drones <span class="fl">0.66626</span></a>
<a class="sourceLine" id="cb85-5" data-line-number="5">dron <span class="fl">0.627708</span></a>
<a class="sourceLine" id="cb85-6" data-line-number="6">microdrone <span class="fl">0.603919</span></a>
<a class="sourceLine" id="cb85-7" data-line-number="7">quadrone <span class="fl">0.603164</span></a>
<a class="sourceLine" id="cb85-8" data-line-number="8">stabilisation <span class="fl">0.594831</span></a>
<a class="sourceLine" id="cb85-9" data-line-number="9">stabilised <span class="fl">0.579854</span></a>
<a class="sourceLine" id="cb85-10" data-line-number="10">naval <span class="fl">0.572352</span></a>
<a class="sourceLine" id="cb85-11" data-line-number="11">piloted <span class="fl">0.571288</span></a>
<a class="sourceLine" id="cb85-12" data-line-number="12">stabilise <span class="fl">0.564452</span></a></code></pre></div>
<p>Hmmm, OK but maybe we should try UAV</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">Query word? uav</a>
<a class="sourceLine" id="cb86-2" data-line-number="2">uavgs <span class="fl">0.768899</span></a>
<a class="sourceLine" id="cb86-3" data-line-number="3">aerial <span class="fl">0.742971</span></a>
<a class="sourceLine" id="cb86-4" data-line-number="4">uavs <span class="fl">0.710861</span></a>
<a class="sourceLine" id="cb86-5" data-line-number="5">unmanned <span class="fl">0.692975</span></a>
<a class="sourceLine" id="cb86-6" data-line-number="6">uad <span class="fl">0.684599</span></a>
<a class="sourceLine" id="cb86-7" data-line-number="7">ua <span class="fl">0.667772</span></a>
<a class="sourceLine" id="cb86-8" data-line-number="8">copter <span class="fl">0.666946</span></a>
<a class="sourceLine" id="cb86-9" data-line-number="9">usv <span class="fl">0.652238</span></a>
<a class="sourceLine" id="cb86-10" data-line-number="10">uas <span class="fl">0.644046</span></a>
<a class="sourceLine" id="cb86-11" data-line-number="11">flight <span class="fl">0.643298</span></a></code></pre></div>
<p>This is printing some words that we would expect in both cases such as plurals (drones, uavs) along with types of drones but we need to investigate some high scoring terms, for exaample in set one we have the word <code>codrone</code> which is a specific make of drone. The word <code>dron</code> may be the word for drone in another language. In the second set we have <code>uavgs</code> which may stand for UAV Ground School along with terms such as <code>uad</code> which stands for unmanned aerial device.</p>
<p>So, this reveals that we are obtaining some meaningful results on single terms that can guide our construction of a search query. We could also look at this another way by identifying terms that may be sources of noise. Here we will use the word bee.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1">Query word? bee</a>
<a class="sourceLine" id="cb87-2" data-line-number="2">honey <span class="fl">0.861416</span></a>
<a class="sourceLine" id="cb87-3" data-line-number="3">hive <span class="fl">0.830507</span></a>
<a class="sourceLine" id="cb87-4" data-line-number="4">bees <span class="fl">0.826173</span></a>
<a class="sourceLine" id="cb87-5" data-line-number="5">hives <span class="fl">0.81909</span></a>
<a class="sourceLine" id="cb87-6" data-line-number="6">honeybee <span class="fl">0.81183</span></a>
<a class="sourceLine" id="cb87-7" data-line-number="7">queen <span class="fl">0.806328</span></a>
<a class="sourceLine" id="cb87-8" data-line-number="8">honeycombs <span class="fl">0.803329</span></a>
<a class="sourceLine" id="cb87-9" data-line-number="9">honeybees <span class="fl">0.784235</span></a>
<a class="sourceLine" id="cb87-10" data-line-number="10">honeycomb <span class="fl">0.783923</span></a>
<a class="sourceLine" id="cb87-11" data-line-number="11">beehives <span class="fl">0.783663</span></a></code></pre></div>
<p>This is yielding decent results. We could for example use this to build a term exclusion list and we might try something similar with the word music (to exclude words like musician, musical, melodic, melody)where it is clear they cannot be linked to drones. For example, there may be drones that play music… but the words musician, melodic and melody are unlikely to be associated with musical drones.</p>
<p>An other way of generating word vectors is using the Continuous Bag of Words (CBOW) methods.</p>
<p>A Continuous Bag of Words approach attempts to predict the target word from the context words that surround it . <!--- Make sure clear about the distinction with this---></p>
<p>The word drone is another name for an unmanned aerial vehicle or UAV.</p>
</div>
<div id="exploring-analogies" class="section level3">
<h3><span class="header-section-number">9.0.6</span> Exploring Analogies</h3>
<p>Word vectors are famous for being able to predict relationships of the type</p>
<p>”King” - ”Man” + ”Woman” = “Queen”</p>
<p>We can experiment with this with the drones vector we created earlier using the <code>analogies</code> function in fasttext.In the <em>terminal</em> run:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>.<span class="op">/</span>fasttext analogies <span class="op">/</span>Users<span class="op">/</span>colinbarnes<span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>drones_vec.bin</a></code></pre></div>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1">Query <span class="kw">triplet</span> (A <span class="op">-</span><span class="st"> </span>B <span class="op">+</span><span class="st"> </span>C)? drone autonomous bee</a>
<a class="sourceLine" id="cb89-2" data-line-number="2">honey <span class="fl">0.687067</span></a>
<a class="sourceLine" id="cb89-3" data-line-number="3">larvae <span class="fl">0.668081</span></a>
<a class="sourceLine" id="cb89-4" data-line-number="4">larva <span class="fl">0.668004</span></a>
<a class="sourceLine" id="cb89-5" data-line-number="5">brood <span class="fl">0.664371</span></a>
<a class="sourceLine" id="cb89-6" data-line-number="6">honeycombs <span class="fl">0.655226</span></a>
<a class="sourceLine" id="cb89-7" data-line-number="7">hive <span class="fl">0.653633</span></a>
<a class="sourceLine" id="cb89-8" data-line-number="8">honeycomb <span class="fl">0.651272</span></a>
<a class="sourceLine" id="cb89-9" data-line-number="9">bees <span class="fl">0.632742</span></a>
<a class="sourceLine" id="cb89-10" data-line-number="10">comb <span class="fl">0.626189</span></a>
<a class="sourceLine" id="cb89-11" data-line-number="11">colonies <span class="fl">0.61386</span></a></code></pre></div>
<p>what this tells us is that drone - automomous + bee = honey or larvae, or larva or brood. We can more or less reverse this calculation.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">Query <span class="kw">triplet</span> (A <span class="op">-</span><span class="st"> </span>B <span class="op">+</span><span class="st"> </span>C)? bee honey drone</a>
<a class="sourceLine" id="cb90-2" data-line-number="2">drones <span class="fl">0.651486</span></a>
<a class="sourceLine" id="cb90-3" data-line-number="3">codrone <span class="fl">0.63545</span></a>
<a class="sourceLine" id="cb90-4" data-line-number="4">stabilisation <span class="fl">0.592739</span></a>
<a class="sourceLine" id="cb90-5" data-line-number="5">dron <span class="fl">0.582929</span></a>
<a class="sourceLine" id="cb90-6" data-line-number="6">na <span class="fl">0.572516</span> <span class="co"># drop NA from the underlying set</span></a>
<a class="sourceLine" id="cb90-7" data-line-number="7">microdrone <span class="fl">0.571889</span></a>
<a class="sourceLine" id="cb90-8" data-line-number="8">naval <span class="fl">0.541626</span></a>
<a class="sourceLine" id="cb90-9" data-line-number="9">continuation <span class="fl">0.54127</span></a>
<a class="sourceLine" id="cb90-10" data-line-number="10">proposition <span class="fl">0.540825</span></a>
<a class="sourceLine" id="cb90-11" data-line-number="11">déstabilisation <span class="fl">0.536781</span></a></code></pre></div>
<p>What this example illustrates is that terms for bees and terms for drones as a technology, as we might expect, occupy different parts of the vector space.</p>
<p>It is fundamentally quite difficult to conceptualise a 100 or 300 dimension vector space. However, Google has developed a <a href="http://projector.tensorflow.org/">tensorflow projector</a> and a video that discusses high dimensional space in an accessible way <a href="https://www.youtube.com/watch?v=wvsE8jm1GzE">A.I. Experiments: Visualizing High-Dimensional Space</a>. The <a href="https://distill.pub/">Distill website</a> offers good examples of the visualisation of a range of machine learning components.</p>
<p>We can view a simplifiled visualisation of the term drones in 200 dimension vector space (on a much larger model than we have discussed above) in Figure <a href="#fig:dronesviz"><strong>??</strong></a>.</p>
<p><img src="images/drones1_embedding.png" width="948" /></p>
<p>Here we have selected to display 400 terms linked to the source word drones across the representation of the vector space. As with network analysis we can see that clusters of association emerge. As we zoom in to the vector space representation we start to more clearly see nearest points based in this case of Principle Components Analysis (PCA).</p>
<p><img src="images/dronesembedding_isolated.png" width="606" /></p>
<p>The representation of terms in vector space in these images is different to those we viewed above and more clearly favours bees and music, although closer inspection reveals words such as ‘winged’, ‘terrorized’. ‘missile’ and ‘predator’ that suggest the presence of news related terms when compared with the patent data used above.</p>
<p>This visualisation highlights the power of the representation of words as vectors in vector space. It also highlights that the vector space is determined by the source data. For example, many vector models are built from downloads of the <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia">content of Wikipedia</a> (available in a number of languages) or on a much larger scale from internet web pages through services such as <a href="https://commoncrawl.org/">Common Crawl</a>.</p>
<p>For patent analytics, this can present the problem that the language in vector models lacks the specificity in termns of the use of technical language of patent documents. For that reason you may be better, as illustrated above, wherever possible it is better to use patent domain specific word embeddings.</p>
</div>
<div id="patent-specific-word-embeddings" class="section level3">
<h3><span class="header-section-number">9.0.7</span> Patent Specific Word Embeddings</h3>
<p>The increasing accessibility of patent data, with both the US and the EP full text collections now available free of charge, has witnessed growing efforts to develop word embedding approaches to patent classification and search.</p>
<p>A very good example of this type of appproach is provided by Julian Risch and Ralf Krestel at the Hasso Plattner Institute at the University of Potsdam with a focus on patent classification <span class="citation">(Risch and Krestel <a href="#ref-lens.org/078-034-591-343-369">2019</a>)</span>.</p>
<p>Risch and Krestel test word embedding approaches using three different datasets</p>
<ol style="list-style-type: lower-alpha">
<li>the WIPO-alpha dataset of 75,000 excerpts of English language PCT applications accompanied by subclass information <span class="citation">(<em>Automated Categorization in the International Patent Classification</em> <a href="#ref-lens.org/056-971-725-855-254">2003</a>)</span>;</li>
<li>A dataset of 5.4 million patent documents from the USPTO between 1976-2016 called USPTO-5M containing the full text and bibliographic data.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>]</li>
<li>A public dataset of with 2 million JSON formatted USPTO patent documents called USPTO-2M created by Jason Hoou containing titles, abstracts and IPC subclasses..<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></li>
</ol>
<p>These datasets are primarilly intended to assist with the automatic classification of patent documents. They used fastText on 5 million patent docunenst to train word enmeddings with 100, 200 and 300 dimensions based on lowercass words occurring 10 or more times and with a context window of 5. This involved a total of 28 billion tokens and, as they rightly point out, this is larger than the english Wikipedia corpus (16 billion) but lower than the 600 billion tokens in the Common Crawl dataset <span class="citation">(Risch and Krestel <a href="#ref-lens.org/078-034-591-343-369">2019</a>)</span>. As part of an oopen access approach focusing on reproducibility the resulting datasets are made available free of charge<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
<p>The word embedding were then used to support the develpment of a deep neural network using gated recurrent units (GRU) with the aim of predicting the main subclass for the doecument using 300 words from the title and abstract of the documents. The same network arcgicetchre was used to test the WIPO set, the standard Wikipedia embeddings. the USPTO-2M set and the USPTO-5M (restriced to titles and abstracts). The specific details of the experiments performed with the word embeddings and GRU deep neural network are available in the article. The main finding of the research is to demonstrate that patent specific word embeddings outperform the Wikipedia based embeddings. This confirms the very crude intuition that we gained from the very small samples of data on the term drones compared with the exploration of wikipedia based emebeddings.</p>
<p>One important challenge with the use of word vectors or embeddings is size. FasText has the considerable advantage that it is designed to run on CPU. That is, as we have seen it can be run on a laptop. However, the word embeddings provided by Risch and Krestel, notably the 300 dimension dataset, may present significant memory challenges to be used in practice. The word embeddings generated by the work of Risch and Krestel demonstrate some of these issues. Thus, the 100 dimension word embeddings vectors are 6 gigabytes in size, the 200 dimensions file is 12Gb and the 300 dimensions is 18Gb. In practice, these file sizes are not as initially intimidating as they appear. Thuse the 6Gb 100 dimension vectors easily run in fasttext on laptop with 16Gb of RAM.</p>
</div>
<div id="machine-learning-in-classification" class="section level3">
<h3><span class="header-section-number">9.0.8</span> Machine learning in Classification</h3>
<p>Classification tasks essentially involve deciding if a particular document or image falls into an Yes or No or Dog or Cat category. In reality, the situation can be more complex than this because a single entity may overlap.</p>
<p>In the case of the term <code>drone</code> in patent data we have already seen that there are basically three categories in whicvh teh term drone may fall</p>
<ol style="list-style-type: lower-alpha">
<li>autonomous vehicles (flying, floating, submarine, space vehicles)</li>
<li>Bee or apiculture related activity</li>
<li>musical</li>
<li>other (that we have not spotted yet)</li>
</ol>
<div id="classification-with-prodigy" class="section level4">
<h4><span class="header-section-number">9.0.8.1</span> Classification with Prodigy</h4>
<p>As a starting point in working with machine</p>
<p>Create a dataset</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>prodigy dataset drones <span class="st">&quot;drones classfication set&quot;</span></a></code></pre></div>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="op">$</span><span class="st"> </span>prodigy terms.teach drones en_core_web_lg  <span class="op">--</span>seeds <span class="st">&quot;drone, drones&quot;</span></a></code></pre></div>
<p><a href="https://support.prodi.gy/t/loading-gensim-word2vec-vectors-for-terms-teach/333/19" class="uri">https://support.prodi.gy/t/loading-gensim-word2vec-vectors-for-terms-teach/333/19</a></p>
<p>INSERT</p>
<p>A problem immediately becomes apparent when using the vectors that accompn</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1">wget <span class="op">-</span>c http<span class="op">:</span><span class="er">//</span>mattmahoney.net<span class="op">/</span>dc<span class="op">/</span>enwik9.zip <span class="op">-</span>P data<span class="op">/</span>fasttext</a>
<a class="sourceLine" id="cb93-2" data-line-number="2">unzip data<span class="op">/</span>fasttext<span class="op">/</span>enwik9.zip <span class="op">-</span>d data<span class="op">/</span>fasttext</a></code></pre></div>
<p>preprocess the XML file and check it in the terminal</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">perl wikifil.pl <span class="op">/</span>Users<span class="op">/</span>colinbarnes<span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>enwik9 <span class="op">&gt;</span><span class="st"> </span><span class="er">/</span>Users<span class="op">/</span>colinbarnes<span class="op">/</span>handbook<span class="op">/</span>data<span class="op">/</span>fasttext<span class="op">/</span>fil9</a></code></pre></div>
<p>Check the file:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1">head <span class="op">-</span>c <span class="dv">80</span> data<span class="op">/</span>fil9</a></code></pre></div>
<p>The vector model can be built either in</p>
<p>”King”) - vector(”Man”) + vec- tor(”Woman”</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-lens.org/056-971-725-855-254">
<p><em>Automated Categorization in the International Patent Classification</em>. 2003. Vol. 37. 1. United States: Association for Computing Machinery (ACM). <a href="https://doi.org/10.1145/945546.945547">https://doi.org/10.1145/945546.945547</a>.</p>
</div>
<div id="ref-bojanowski2016enriching">
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. “Enriching Word Vectors with Subword Information.” <em>arXiv Preprint arXiv:1607.04606</em>.</p>
</div>
<div id="ref-joulin2016fasttext">
<p>Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. “FastText.zip: Compressing Text Classification Models.” <em>arXiv Preprint arXiv:1612.03651</em>.</p>
</div>
<div id="ref-joulin2016bag">
<p>Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. “Bag of Tricks for Efficient Text Classification.” <em>arXiv Preprint arXiv:1607.01759</em>.</p>
</div>
<div id="ref-lens.org/078-034-591-343-369">
<p>Risch, Julian, and Ralf Krestel. 2019. “Domain-Specific Word Embeddings for Patent Classification.” <em>Drug Testing and Analysis</em> 53 (1): 108–22. <a href="https://doi.org/10.1108/dta-01-2019-0002">https://doi.org/10.1108/dta-01-2019-0002</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p><a href="https://royalsociety.org/-/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">https://royalsociety.org/-/media/policy/projects/machine-learning/publications/machine-learning-report.pdf</a><a href="machinelearning.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z">https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z</a><a href="machinelearning.html#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p><a href="https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf" class="uri">https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf</a><a href="machinelearning.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p><a href="https://www.uspto.gov/learning-and-resources/bulk-data-products">USPTO Bulk Products</a>, now more readily available from <a href="http://www.patentsview.org/download/">PatentsView</a><a href="machinelearning.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>Available for download from Github at <a href="https://github.com/JasonHoou/USPTO-2M">https://github.com/JasonHoou/USPTO-2M</a> and at <a href="http://mleg.cse.sc.edu/DeepPatent/">http://mleg.cse.sc.edu/DeepPatent/</a><a href="machinelearning.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>Accessible from: <a href="https://hpi.de/naumann/projects/web-science/deep-learning-for-text/patent-classification.html">https://hpi.de/naumann/projects/web-science/deep-learning-for-text/patent-classification.html</a>, last accessed: 2019-09-17<a href="machinelearning.html#fnref23" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="references.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["handbook2.pdf", "handbook2.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
